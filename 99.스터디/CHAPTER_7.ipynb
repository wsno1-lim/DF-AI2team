{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5bh718V7JHt8"
   },
   "source": [
    "# CHAPTER 7 금융에서의 딥러닝\n",
    "--------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W8L6lK2l_90t"
   },
   "source": [
    "* 머신러닝과 딥러닝의 차이\n",
    "  - 금융 데이터 분석에 딥러닝을 활용하는 이유\n",
    "  - 딥러닝의 발전과 역사\n",
    "  - 금융데이터 활용에 유용한 딥러닝 알고리즘, 연구현황\n",
    "* 딥러닝 전략 개발에 필요한 케라스\n",
    "  - 논문 구현을 통한 딥러닝 알고리즘 활용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1wnJX7AnCLK0"
   },
   "source": [
    "## 7.1 딥러닝\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6C7TfCVrCTRq"
   },
   "source": [
    "* 딥러닝?\n",
    "  - 머신러닝의 특화된 종류\n",
    "  - 머신러닝 내 표현학습 범주에 속함\n",
    "  - 인간의 뇌에서 일어나는 의사결정 과정을 모방해 작동하는 인공 신경망\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0PTA-jUXCUWo"
   },
   "source": [
    "### 7.1.1 딥러닝과 머신러닝의 차이점\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 각 층을 거치며 점진적으로 더 복잡한 표현이 만들어진다.\n",
    "2. 이런 점진적인 중간 표현이 공동으로 학습된다.\n",
    "\n",
    "> 특성공학 : 데이터의 복잡성을 줄이고 학습 알고리즘에서 패턴을 뚜렷하게 나타내는 과정\n",
    "\n",
    "> 딥러닝 : 복잡한 Task ==> 추상적인 Task로 변경"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JK9ltTraCUeK"
   },
   "source": [
    "* 머신러닝과 딥러닝\n",
    "> 규칙기반 시스템 : 출력(결과) -> 직접 디자인한 프로그램 -> 입력\n",
    "\n",
    "> 전통적인 머신러닝 : 출력(결과) -> 특성으로부터 매핑 -> 직접 디자인한 특성 - > 입력 \n",
    "\n",
    "> 표현학습 : \n",
    "> 입력 -> 특성 -> 특성으로부터 매핑 -> 출력(결과)\n",
    "  \n",
    "> 입력 -> 간단한 특성 -> 추상적인 특성의 추가적인 레이어 -> 특성으로부터 매핑 -> 출력(결과)\n",
    "  그림7.1 참고\n",
    "  \n",
    "- 딥러닝은 사람이 찾아야할 목적에 적합한 특성을 사람 대신 자동으로 찾아줌\n",
    "  \n",
    "> 그림 7-3,4 분류 방법의 차이\n",
    "\n",
    "* 단순함의 차이 \n",
    "> 딥러닝 : 고도의 다단계 작업 과정을 엔드-투-엔드 모델로 만듬, 데이터만 넣으면 해결\n",
    "> 머신러닝 : 문제 해결 시 주로 문제를 여러 조각으로 쪼갬, 각각에 대한 해답을 구해 병합함\n",
    "\n",
    "* 다용도와 재사용성\n",
    "> 딥러닝 : 처음부터 다시 시작하지 않고 추가 훈련가능, 사전 학습된 모델의 가중치를 새로운 모델에 적용 가능\n",
    "완전연결층 만 변경해 분류를 실행할 수 있는 전이학습 가능\n",
    "\n",
    "> 단점 : \n",
    "> 1. 하드웨어 의존도가 높다\n",
    "> 2. 데이터 의존도 가 높다 (그림 7-6)\n",
    "> 3. 실행 시간이 길다.\n",
    "> 4. 해석력이 약하다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ssxGA8aSKmEZ"
   },
   "source": [
    "### 7.1.2 딥러닝의 발전 과정\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ACYSnNjKpna"
   },
   "source": [
    "그림 7-7\n",
    "> 1950년 대 신경망 기술 탄생\n",
    "> 당시 딥러닝이 아닌 퍼셉트론으로 명명\n",
    "> 1969년 마빈 민스키(저서:퍼센트론스) - 단층 퍼셉트론은 비선형 함수를 표한할수 있고, 다층 퍼셉트론이라해도 좋은 성능을 내기 어렵다 주장\n",
    " 첫번째 겨울 시작...\n",
    " \n",
    "> 1980년대 제프리 힌튼, 얀트쿤 - 오차역전파 알고리즘 프로토타입\n",
    "> 1986년 Learning Representations by Back propagatiogs Errors 눈문 발표, 오차역전파를 이용한 다층 신경망 훈련 가능\n",
    "\n",
    "> 1990년대 중반 통계학습이론과 서포트 벡터 머신의 발전, 신경망 학습에 대한 이론 부족, 과도한 계산량 등 다시 침체기\n",
    "\n",
    "> 2010년대 컴퓨터 발전 및 빅데이터 출현으로 부흥기 도래"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CXLiywdcM-VS"
   },
   "source": [
    "### 7.1.3 금융 데이터 분석에 딥러닝을 활용하는 이유\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ShtMUgiEM-c-"
   },
   "source": [
    "* 프란체스카 라제리 - 딥러닝을 시계열 분석에 사용해야하는 이유\n",
    "> 첫째, 딥러닝은 정제되지 않거나 불완전한 데이터로부터 자동으로 특성을 학습하고 추출할 수 있다.\n",
    "  고정된 임시 의존 관계 -> 데이터 정상성 확보, 표현 벡터(오토인코더, GAN), 임베딩(NLP) 등\n",
    "  \n",
    "> 둘째, 딥러닝은 다중 입력과 출력을 지원한다. 시계열 데이터 분석에서 다양한 방식의 입력/출력을 지원한다는 점은 매우 중요하다\n",
    "\n",
    "> 셋째, 딥러닝 네트워크는 비교적 길이가 긴 시퀀스에 걸쳐있는 패턴을 추출하는 데 탁월하다.\n",
    "  순환 신경망(RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kmH1bHO3PTQ9"
   },
   "source": [
    "### 7.1.4 딥러닝 알고리즘 소개\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ngXK9ga4PWGL"
   },
   "source": [
    "* 합성곱 신경망\n",
    "> CNN에서 C는 합성곱을 의미, 합성곱층의 목적 => 입력 이미지에서 특성을 추출하는 것  \n",
    "\n",
    "- 그림 7-8\n",
    "\n",
    "> 정상성은 시계열 데이터의 통계적 특성이 시간이 지나도 변하지 않는다는 뜻\n",
    "\n",
    "> 픽셀의 종속성 - 이미지에서 한 점과 의미 있게 정상성이 아니라 위치에 대한 정상성인데, 동일한 패턴들이 반복되는 특성을 잡을 수 있음\n",
    "\n",
    "> 파라미터 공유 - 특성 지도 하나를 출력하기 위해 필터를 단 한 장만 유지하기 때문에 완전 연결층보다 훨씬 더 적은 파라미터 개수를 사용해 메모리를 아낄수 있어 연산량이 적고, 통계적 효율이 향상됨\n",
    "\n",
    "> 이동 불변성 - 입력 위치가 변해도 출력이 변하지 않음\n",
    "- CNN의 원천 입력 데이터로부터 복잡하고 추상적인 시각적 개념을 효과적으로 추출하는 특징은 시계열 예측문제에도 적용 될 수 있음.\n",
    "\n",
    "* 순환 신경망\n",
    "> RNN은 현재/이전 입력값 고려함 (시계열에 적합)\n",
    "\n",
    "> CNN은 메모리가 없는 피드 포워크 네트워크라 하나의 데이터 포인트로 변환해야 하고, 개별적으로 처리되어 이전 입력 영향을 받지 않음\n",
    "\n",
    "> LSTM : 데이터가 너무 길어져서 신경망이 깊어지는 문제해결용도\n",
    ">       역전파라는 방법을 통해 학습, 셀스테이트라는 개념 -> 내부에 있는 게이트들을 통해 어떤 정보를 기억하고, 어떤 정보를 버릴지 추가적 학습 가능, 노이즈 제거 시도"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> GRU(Gated Recurent Unit) : LSTM 의 간소화 버전이나 아직 차이를 밝히지 못함\n",
    "\n",
    "-  LSTM에서는 출력, 입력, 삭제 게이트라는 3개의 게이트가 존재. \n",
    "- 반면, GRU에서는 업데이트 게이트와 리셋 게이트 두 가지 게이트만이 존재. \n",
    "- GRU는 LSTM보다 학습 속도가 빠르다고 알려져있지만 여러 평가에서 \n",
    "- GRU는 LSTM과 비슷한 성능을 보인다고 알려져 있음.\n",
    "\n",
    "https://wikidocs.net/22889\n",
    "\n",
    "- 실제 GRU 은닉층을 추가하는 코드.\n",
    "> model.add(GRU(hidden_size, input_shape=(timesteps, input_dim)))\n",
    "\n",
    "> ARIMA, SARIMA\n",
    "https://ahnyuk2.blogspot.com/2019/07/time-series-arima-sarima.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 비지도 학습\n",
    "> 오토인코더 : 특징 백터x를 입력받아 동일하거나 유사한 벡터X를 출력하는 신경망\n",
    "\n",
    "> 출력 노드 = 입력 노드\n",
    "\n",
    "> 비지도 학습 알고리즘 --> 선형/비선형관계, 잡음 제거에 탁월, 자료 분표 패턴 등 추정\n",
    "\n",
    "> 입력차원축소인 PCA와 비슷하나, 다른 네트워크에서 재사용 가능\n",
    "\n",
    "* 생성 모델\n",
    "> 훈련 데이터가 주어졌을 때 해당 데이터가 가지는 실제 분포와 같은 분포에서 샘플링된 값으로 새로운 데이터를 생성하는 모델 (GAN, VAE, RNN, 제한된 볼츠만 머신)\n",
    "> GAN : 데이터 확장에 활용 --> 비슷한 데이터를 만들어 다시 입력으로 활용하는 것\n",
    "- ※ GAN(Generative Adversarial Network, 생성 대립적 신경망) : 생성모델과 판별모델이 경쟁하면서 실제와 가까운 이미지, 동영상, 음성 등을 자동으로 만들어내는 기계학습\n",
    "\n",
    "\n",
    "* 자연어 처리\n",
    "> NLP : 실제로 트윗 분석을 통해 펀드 투자 성과사례가 있음\n",
    "> NLP가 딥러닝 기술과는 다르지만\n",
    "> 구글트렌드나 트윗 데이터를 이용한 감성 분석에 활용됨.\n",
    "> 텍스트 데이터 수집/분석, 활용이라는 프로세스 필요 ==> 유의미한 데이터 제공하는 곳이 필요(블룸버그, 로이터, 금융 미디어, 데이터 플랫폼 상품"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uAFKmzcqPkDV"
   },
   "source": [
    "### 7.1.5 딥러닝을 금융 시장 데이터에 적용한 연구\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8vbXoQ2pQo20"
   },
   "source": [
    "* Financial Time Series Forecasting with Deep Learning : A Systematic Literature Review : 2005-2019\n",
    "  - 2017년 부터 폭발적 증가\n",
    "  - 딥러닝을 활용한 금융 데이터 분석에서 선택한 모델 유형별 비율 : RNN의 비율 51%\n",
    "  - 논문에서 사용한 RNN 계열 모델 비율 LTSM : 60.4%, RNN : 29.7%, GRU : 9.89%\n",
    "\n",
    "https://quantpedia.com/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 딥러닝 알고리즘 구현을 위한 케라스"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.1 케라스 소개"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 텐서플로, 파이토치,케라스\n",
    "> 케라스 : 텐서플로나 씨아노 CNTK 등 저수준 라이브러리를 감싸는 고수준 API\n",
    "> CPU, GPU 연산을 동일한 코드로 사용할 수 있게 지원"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "57qtSC5QRe58"
   },
   "source": [
    "### 7.2.2 주요 모듈과 함수\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u0yySTVBRk_W"
   },
   "source": [
    "* 표 7-1 p261\n",
    "> 참조"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1kijrETCbQiB"
   },
   "source": [
    "### 7.2.3 예제로 살펴보는 케라스 활용법\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://light-tree.tistory.com/141\n",
    "rmsprop 파라미터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "eroqVtZ_bQ_U"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "32/32 [==============================] - 0s 656us/step - loss: 0.7068 - accuracy: 0.5010\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 0s 656us/step - loss: 0.6963 - accuracy: 0.5270\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 0s 718us/step - loss: 0.6931 - accuracy: 0.5440\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.6884 - accuracy: 0.5470\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 0s 843us/step - loss: 0.6854 - accuracy: 0.5610\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 0s 937us/step - loss: 0.6828 - accuracy: 0.5780\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 0s 812us/step - loss: 0.6785 - accuracy: 0.5680\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 0s 843us/step - loss: 0.6750 - accuracy: 0.5860\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 0s 874us/step - loss: 0.6703 - accuracy: 0.6070\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 0s 781us/step - loss: 0.6678 - accuracy: 0.6010\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# 전체 데이터셋을 생성한다. 데이터셋 중에서도 특성 데이터를 만드는 것. 입력 데이터의 형태는 (1000,100)인데 이는 1000개의 샘플과 100개의 특성 데이터를 뜻함\n",
    "data = np.random.random((1000,100))\n",
    "# 레이블 데이터를 생성. 레이블 데이터 역시 랜덤한데 이진 분류를 위해 0과 1로 나뉘도록 랜덤하지만 제약을 두고 생성\n",
    "labels = np.random.randint(2,size=(1000,1))\n",
    "\n",
    "# Sequential() 객체를 받아 모델을 만듬. 케라스에서 모델을 만드는 두 가지 방식 중 층을 선형으로 쌓는 모델\n",
    "model = Sequential()\n",
    "# model 인스턴스의 add() 함수를 호출해 순차적으로 층을 추가, Dense() 객체는 완전 연결층을 의미. \n",
    "# 그리고 모델의 첫 번째 층에만 해당하는 입력 데이터의 정보를 제공\n",
    "# 모델에 어떤 입력 형태를 전달할지 알려줘야함\n",
    "# 특성 개수가 100개 이므로 input_dim 매개변수에 100을 전달\n",
    "# 케라스에서 활성화 함수를 층에 추가할 떄 매개변수로 추가 가능, 따로 호출해도 가능, 여기서는 Relu 추가\n",
    "model.add(Dense(32, activation = 'relu', input_dim=100))\n",
    "# 최종 출력층 : 분류모델인경우 다중 -> softmax(), 이진 분류 -> sigmoid() 활용\n",
    "model.add(Dense(1,activation = 'sigmoid'))\n",
    "\n",
    "# 컴파일 : 손실함수, 최적화 방식, 성능 측정 지표 설정\n",
    "model.compile(optimizer = 'rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 훈련 : 훈련 데이터를 모델에 전달하고 훈련 횟수 설정\n",
    "history = model.fit(data,labels,epochs=10,batch_size=32)\n",
    "\n",
    "# 데이터모델을 전달해 예측값 생성\n",
    "predictions = model.predict(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(12, kernel_initializer = 'uniform', activation = 'relu', input_dim=8))\n",
    "model.add(Dense(8, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "model.add(Dense(1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
    "\n",
    "model.compile(optimizer = 'adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 그림 7-19 다층 신경망\n",
    "> 은닉층이 한 개 이상 추가된 신경망은 다층 퍼셉트론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gbrCtBzxp16z"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CHAPTER 4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
